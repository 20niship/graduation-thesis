##  サーベイ論文 第一項
模倣学習（Imitation Learning）は、人間のエキスパートの行動を模倣することでエージェントを訓練する機械学習の手法です。GAIL（Generative Adversarial Imitation Learning）や逆強化学習（Inverse Reinforcement Learning）は、模倣学習の一種であり、有名な論文がいくつか存在します。以下に代表的な論文を取り上げて説明します。

1. "Generative Adversarial Imitation Learning" (GAIL) by Jonathan Ho and Stefano Ermon (2016):
この論文では、GAILという手法が提案されました。GAILは、エキスパートのデモンストレーションから学習し、そのデモンストレーションを再現するようにエージェントを訓練する手法です。GAILは、生成モデルと強化学習の枠組みを組み合わせ、エージェントの学習を行います。Generative Adversarial Networks（GAN）を使用して、エキスパートの行動とエージェントの行動を比較し、エージェントの行動をよりエキスパートに近づけるように学習します。

2. "Apprenticeship Learning via Inverse Reinforcement Learning" by Pieter Abbeel and Andrew Y. Ng (2004):
この論文では、逆強化学習（IRL）という概念が紹介されました。逆強化学習は、エキスパートのデモンストレーションから目的関数（報酬関数）を逆推定することで、エージェントの行動を学習する手法です。逆強化学習では、エキスパートの行動を再現するだけでなく、エキスパートの行動背後にある意図や目的を理解することができます。

3. "End-to-End Training of Deep Visuomotor Policies" by Sergey Levine et al. (2016):
この論文では、GAILの一種であるDAGGER（Dataset Aggregation）という手法が紹介されました。DAGGERは、エキスパートのデモンストレーションとエージェントの行動を組み合わせて新たなデータセットを作成し、それを用いてエージェントを再学習する手法です。DAGGERは反復的なプロセスであり、エージェントがエキスパートのパフォーマンスに近づくようになります。

これらの論文は、模倣学習における重要な手法とアルゴリズムを提案しています。

それぞれの手法は、エキスパートの行動を模倣するために異なるアプローチやモデルを使用していますが、共通の目標はエキスパートの知識をエージェントに伝えることです。


ChatGPT

対話型模倣学習（Interactive Imitation Learning、IIL）は、ロボットの実行中に断続的に人間からフィードバックを受けることで、ロボットの振る舞いをオンラインで改善する模倣学習（Imitation Learning、IL）の一分野です。

近年、IILは複雑なロボットタスクの解決において有望なデータ駆動型の代替手法として注目されています。IILの利点は2つあります。まず、人間からのフィードバックによって改善された振る舞いに直接ロボットを導くため、データ効率が高いという点です（これに対して強化学習（Reinforcement Learning、RL）では振る舞いを試行錯誤で発見する必要があります）。次に、フィードバックを学習者の軌跡に直接提供することで、教師と学習者の軌跡の分布の不一致を最小限に抑えるため、頑健性があります（これに対して行動クローニングなどのオフラインのIL手法では異なります）。

しかしながら、IILが提示する機会にも関わらず、その用語や構造、応用可能性は文献では明確で統一されておらず、その発展や革新的な形式や発見の研究が遅れています。

本稿では、IILの研究を容易にし、新たな実践者の参入障壁を下げるために、この分野についての調査結果を提供し統一化・構造化を図ろうとします。さらに、IILの潜在的な可能性や達成された成果、未解決の研究課題についての認識を高めることを目指します。


既存のロボット技術は、主に専門のプログラマーによって使用され、新しい要件にシステムを適応させることはできますが、非専門の労働者やエンドユーザーにとっては柔軟性や適応性に欠けています。模倣学習（Imitation Learning、IL）は、あらゆるユーザーが簡単にロボットや仮想エージェントの振る舞いをプログラムできる可能性のある方向性として、かなりの注目を集めています。教示プロセスは、応用コンテキストで直接行われ、人間にとって自然な方法であり、異なるシナリオごとに振る舞いを適応させるためのエンジニアリングの努力は必要ありません。

もし教師（つまり、タスクに関する知識を持つ人間）が利用可能であり、その知識をエージェントに伝達できる場合、他の機械学習（Machine Learning、ML）手法（たとえば強化学習（Reinforcement Learning、RL）など）ではなく、録画されたデモンストレーションから振る舞いをプログラムすることが望まれます。なぜなら、他の手法は設計、インフラ、安全性、データ効率の課題があり（Sutton and Barto、2018）、時間やリソースの制約により物理システムには適用できない場合が多いからです。

私たち人間が複雑なスキル（たとえば、高速な動力学や器用さを必要とするスキル）を他の人に教える際のように、ロボットを自然な方法でプログラムする可能性は、後でポリシーモデルに適合させるためのデモンストレーションの録画に制限されるわけではありません（Argall et al.、2009）。実際には、非常に単純で簡単なタスクを人から人へ教えるためには、最初の一連のデモンストレーションや指示が十分であることが多いです。例えば、ドアを開けるための指示、電話の充電器を差し込むための指示、日常的に使用するデバイスのユーザーガイドなどです。ただし、スポーツをするなどの複雑な

スキルの場合は、学習のために相互作用のループが必要となります。その場合、教師は生徒に対して直接的に修正や評価を行いながら、過去のミスや成功から振る舞いを改善するための指示を行います。そうでない場合、教師と生徒の両方にとって、事前にすべての可能なシナリオを考慮して説明することは困難になります。

この種の教え方は、デモンストレーション、断続的な修正、評価（採点）といったさまざまなタイプの教示フィードバックに基づいています。例えば、テニスのような複雑なスキルを教える場合、さまざまなステップが関与することがあります。教師は自分自身のストロークの完全なデモンストレーションを生徒に見せます。生徒がその例を再現しようとすると、教師はより良い実行方法を示すことができます。生徒がストロークを行った後、教師は音声の指示で角度や速度、力の微調整を助言することができます。さらに、教師は生徒を断片的に褒めたり、いくつかの決定があまり良くなかったことを明示することができます。このような対話的な教え方は、人間にとってより複雑なスキルを教えるための最も自然な戦略の一つであり、同じ方法でロボットを教えることが望ましいです。

近年、ロボティクスと機械学習の領域では、このような対話的な教え方の戦略がますます採用され、開発されています。本稿では、連続的な意思決定システムのトレーニングのために教師を学習ループに組み込むすべての手法を指して、IILという用語を使用します。この論文の目的は、これらの手法に関する文献を調査し、最も関連性の高い観察結果を整理した形で紹介することです。


## Final

## はじめに
従来のロボット制御技術は、主に特定の要件に合わせて作られたプログラムやシステムによって制御され、柔軟性や適応性にかけているという問題がありました。しかし、近年の機械学習アルゴリズムを使用することで、ロボットが環境を柔軟に理解しより適切な行動を選択することができるようになり、さまざまなタスクの解決において驚くべき成果が得られています。

本稿では、ロボティクスの制御技術に用いられる機械学習アルゴリズムについての包括的なサーベイを述べます。ロボットは、知覚情報をもとに適切な行動を選択する必要があります。機械学習アルゴリズムは、制御システムの設計や最適化において重要な役割を果たしています。また、モデル予測制御や最適制御においても、機械学習アルゴリズムが効果的に活用されています。強化学習は環境のモデルを使用して学習モデルベースのものと、環境モデルを使わずに学習するモデルフリーの二種類に大別され、本稿ではモデルフリーのものに焦点をあてます。

さらに、学習に関連する問題にも触れます。ロボットは、環境やタスクに適応するために学習する必要があります。機械学習アルゴリズムは、ロボットがデータからパターンを学習し、適切な行動を取るためのモデルやポリシーを構築するのに役立ちます。例えば、教師あり学習や教師なし学習などの手法が、ロボットの学習に応用されています。


## これまでの強化学習

### 価値ベース
価値ベースの強化学習は、行動価値関数を使用して最適な行動の評価と選択を行う手法です。以下に、いくつかの代表的な価値ベースの強化学習アルゴリズムの歴史をまとめます。

DQN（Deep Q-Network）は、強化学習の一種であり、ディープラーニング（Deep Learning）を使用してQ学習（Q-Learning）アルゴリズムを拡張した手法です。DQNは、価値関数であるQ関数を近似するために、ディープニューラルネットワーク（Deep Neural Network）を使用します。

Q関数は、ある状態と行動のペアに対してその行動の価値を表す関数です。Q学習は、環境と相互作用しながら最適な行動を学習するためにQ関数を更新する手法です。通常、Q学習では、Q関数をテーブルとして表現し、状態と行動の組み合わせに対して値を保持します。しかし、状態や行動の空間が大きい場合や連続的な状態空間の場合には、テーブルを使用することが困難になります。

DQNでは、ディープニューラルネットワークを使ってQ関数を近似します。ディープニューラルネットワークは、高次元の入力（状態）を受け取り、出力として各行動のQ値を予測します。DQNの学習は、エージェントが環境とやり取りしながらデータを収集し、そのデータを使ってディープニューラルネットワークを更新することで行われます。具体的には、ディープニューラルネットワークのパラメータを勾配降下法を用いて最適化し、Q関数の近似精度を向上させます。

DQNの特徴的な要素は、Experience Replayと呼ばれるテクニックです。Experience Replayでは、エージェントが過去の経験を保存し、ランダムにサンプリングしてディープニューラルネットワークの学習に使用します。これにより、データの再利用が可能となり、学習の効率と安定性が向上します。

// TODO
〇〇の論文では、DQNを使用して

### Rainbow

### Agent57

### DDPG

### NAF

### GPS


#### 方策ベース (方策ベース＋価値ベース)
方策勾配法

Actor Critic

soft actor critic


## transformerベース

### Deep inverse reinforcement Learning

### Generalization

### interactive imitation learning



## 参考文献
- ![](https://www.researchgate.net/publication/364987581/figure/fig2/AS:11431281094048837@1667358903624/Relationship-between-different-sets-of-learning-paradigms-related-to-the-scope-of-this.png)
  - https://www.researchgate.net/publication/364987581_Interactive_Imitation_Learning_in_Robotics_A_Survey
- A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation
   - https://arxiv.org/pdf/1612.07139.pdf
- https://www.frontiersin.org/articles/10.3389/frobt.2021.777363/full
- Interactive Imitation Learning in Robotics: A Survey
  - https://arxiv.org/abs/2211.00600
- https://www.skillupai.com/blog/tech/dl-papers-2022-4/
- http://karel.tsuda.ac.jp/lec/DeepLearning/dl_book19/colab/dl_book19_ch10.html