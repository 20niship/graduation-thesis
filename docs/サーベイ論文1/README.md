##  サーベイ論文 第一項
模倣学習（Imitation Learning）は、人間のエキスパートの行動を模倣することでエージェントを訓練する機械学習の手法です. GAIL（Generative Adversarial Imitation Learning）や逆強化学習（Inverse Reinforcement Learning）は、模倣学習の一種であり、有名な論文がいくつか存在します. 以下に代表的な論文を取り上げて説明します. 

1. "Generative Adversarial Imitation Learning" (GAIL) by Jonathan Ho and Stefano Ermon (2016):
この論文では、GAILという手法が提案されました. GAILは、エキスパートのデモンストレーションから学習し、そのデモンストレーションを再現するようにエージェントを訓練する手法です. GAILは、生成モデルと強化学習の枠組みを組み合わせ、エージェントの学習を行います. Generative Adversarial Networks（GAN）を使用して、エキスパートの行動とエージェントの行動を比較し、エージェントの行動をよりエキスパートに近づけるように学習します. 

2. "Apprenticeship Learning via Inverse Reinforcement Learning" by Pieter Abbeel and Andrew Y. Ng (2004):
この論文では、逆強化学習（IRL）という概念が紹介されました. 逆強化学習は、エキスパートのデモンストレーションから目的関数（報酬関数）を逆推定することで、エージェントの行動を学習する手法です. 逆強化学習では、エキスパートの行動を再現するだけでなく、エキスパートの行動背後にある意図や目的を理解することができます. 

3. "End-to-End Training of Deep Visuomotor Policies" by Sergey Levine et al. (2016):
この論文では、GAILの一種であるDAGGER（Dataset Aggregation）という手法が紹介されました. DAGGERは、エキスパートのデモンストレーションとエージェントの行動を組み合わせて新たなデータセットを作成し、それを用いてエージェントを再学習する手法です. DAGGERは反復的なプロセスであり、エージェントがエキスパートのパフォーマンスに近づくようになります. 

これらの論文は、模倣学習における重要な手法とアルゴリズムを提案しています. 

それぞれの手法は、エキスパートの行動を模倣するために異なるアプローチやモデルを使用していますが、共通の目標はエキスパートの知識をエージェントに伝えることです. 


ChatGPT

対話型模倣学習（Interactive Imitation Learning、IIL）は、ロボットの実行中に断続的に人間からフィードバックを受けることで、ロボットの振る舞いをオンラインで改善する模倣学習（Imitation Learning、IL）の一分野です. 

近年、IILは複雑なロボットタスクの解決において有望なデータ駆動型の代替手法として注目されています. IILの利点は2つあります. まず、人間からのフィードバックによって改善された振る舞いに直接ロボットを導くため、データ効率が高いという点です（これに対して強化学習（Reinforcement Learning、RL）では振る舞いを試行錯誤で発見する必要があります）. 次に、フィードバックを学習者の軌跡に直接提供することで、教師と学習者の軌跡の分布の不一致を最小限に抑えるため、頑健性があります（これに対して行動クローニングなどのオフラインのIL手法では異なります）. 

しかしながら、IILが提示する機会にも関わらず、その用語や構造、応用可能性は文献では明確で統一されておらず、その発展や革新的な形式や発見の研究が遅れています. 

本稿では、IILの研究を容易にし、新たな実践者の参入障壁を下げるために、この分野についての調査結果を提供し統一化・構造化を図ろうとします. さらに、IILの潜在的な可能性や達成された成果、未解決の研究課題についての認識を高めることを目指します. 


既存のロボット技術は、主に専門のプログラマーによって使用され、新しい要件にシステムを適応させることはできますが、非専門の労働者やエンドユーザーにとっては柔軟性や適応性に欠けています. 模倣学習（Imitation Learning、IL）は、あらゆるユーザーが簡単にロボットや仮想エージェントの振る舞いをプログラムできる可能性のある方向性として、かなりの注目を集めています. 教示プロセスは、応用コンテキストで直接行われ、人間にとって自然な方法であり、異なるシナリオごとに振る舞いを適応させるためのエンジニアリングの努力は必要ありません. 

もし教師（つまり、タスクに関する知識を持つ人間）が利用可能であり、その知識をエージェントに伝達できる場合、他の機械学習（Machine Learning、ML）手法（たとえば強化学習（Reinforcement Learning、RL）など）ではなく、録画されたデモンストレーションから振る舞いをプログラムすることが望まれます. なぜなら、他の手法は設計、インフラ、安全性、データ効率の課題があり（Sutton and Barto、2018）、時間やリソースの制約により物理システムには適用できない場合が多いからです. 

私たち人間が複雑なスキル（たとえば、高速な動力学や器用さを必要とするスキル）を他の人に教える際のように、ロボットを自然な方法でプログラムする可能性は、後でポリシーモデルに適合させるためのデモンストレーションの録画に制限されるわけではありません（Argall et al.、2009）. 実際には、非常に単純で簡単なタスクを人から人へ教えるためには、最初の一連のデモンストレーションや指示が十分であることが多いです. 例えば、ドアを開けるための指示、電話の充電器を差し込むための指示、日常的に使用するデバイスのユーザーガイドなどです. ただし、スポーツをするなどの複雑な

スキルの場合は、学習のために相互作用のループが必要となります. その場合、教師は生徒に対して直接的に修正や評価を行いながら、過去のミスや成功から振る舞いを改善するための指示を行います. そうでない場合、教師と生徒の両方にとって、事前にすべての可能なシナリオを考慮して説明することは困難になります. 

この種の教え方は、デモンストレーション、断続的な修正、評価（採点）といったさまざまなタイプの教示フィードバックに基づいています. 例えば、テニスのような複雑なスキルを教える場合、さまざまなステップが関与することがあります. 教師は自分自身のストロークの完全なデモンストレーションを生徒に見せます. 生徒がその例を再現しようとすると、教師はより良い実行方法を示すことができます. 生徒がストロークを行った後、教師は音声の指示で角度や速度、力の微調整を助言することができます. さらに、教師は生徒を断片的に褒めたり、いくつかの決定があまり良くなかったことを明示することができます. このような対話的な教え方は、人間にとってより複雑なスキルを教えるための最も自然な戦略の一つであり、同じ方法でロボットを教えることが望ましいです. 

近年、ロボティクスと機械学習の領域では、このような対話的な教え方の戦略がますます採用され、開発されています. 本稿では、連続的な意思決定システムのトレーニングのために教師を学習ループに組み込むすべての手法を指して、IILという用語を使用します. この論文の目的は、これらの手法に関する文献を調査し、最も関連性の高い観察結果を整理した形で紹介することです. 


## Final

## はじめに
従来のロボット制御技術は、主に特定の要件に合わせて作られたプログラムやシステムによって制御され、柔軟性や適応性にかけているという問題がありました. しかし、近年の機械学習アルゴリズムを使用することで、ロボットが環境を柔軟に理解しより適切な行動を選択することができるようになり、さまざまなタスクの解決において驚くべき成果が得られています. 

本稿では、ロボティクスの制御技術に用いられる機械学習アルゴリズムについての包括的なサーベイを述べます. ロボットは、知覚情報をもとに適切な行動を選択する必要があります. 機械学習アルゴリズムは、制御システムの設計や最適化において重要な役割を果たしています. また、モデル予測制御や最適制御においても、機械学習アルゴリズムが効果的に活用されています. 強化学習は環境のモデルを使用して学習モデルベースのものと、環境モデルを使わずに学習するモデルフリーの二種類に大別され、本稿ではモデルフリーのものに焦点をあてます. 

さらに、学習に関連する問題にも触れます. ロボットは、環境やタスクに適応するために学習する必要があります.  
// TODO:

## これまでの強化学習

### 価値ベース
価値ベースの強化学習は、行動価値関数を使用して最適な行動の評価と選択を行う手法です. 以下に、いくつかの代表的な価値ベースの強化学習アルゴリズムの歴史をまとめます. 

Q学習は、強化学習の一種であり、エージェントが環境との相互作用を通じて最適な行動を学習するための手法です. 通常、Q学習では、Q関数をテーブルとして表現し、状態と行動の組み合わせに対して値を保持します. しかし、状態や行動の空間が大きい場合や連続的な状態空間の場合には、テーブルを使用することが困難になるなどの問題が生じたため、DQN（Deep Q-Network）では深層学習を用いて価値関数であるQ関数を近似する手法が提案されました. DQNの特徴的な要素は、Experience Replayと呼ばれるテクニックです. Experience Replayでは、エージェントが過去の経験を保存し、ランダムにサンプリングしてディープニューラルネットワークの学習に使用します. これにより、データの再利用が可能となり、学習の効率と安定性が向上します. 

DDPG (Deep Deterministic Policy Gradient)は、連続的な行動空間を持つ問題において、価値関数と方策関数を近似するために深層ニューラルネットワークを使用する強化学習アルゴリズムです. DDPGは、連続的な制御タスクにおいて安定した収束性と高い性能を示すことが知られています. 

Rainbowは、強化学習において重要な役割を果たすいくつかの手法を組み合わせた統合的なアルゴリズムです. これには、深層強化学習アルゴリズムであるDQNや、優先度ベースの経験再生、分散学習、デュエルネットワーク、二重Q学習などが含まれています. Rainbowは、一般的な強化学習の課題であるサンプル効率性や安定性の向上に貢献しました. 

Agent57は、Atari 2600のゲームをプレイするエージェントを訓練するための強化学習アルゴリズムです. Agent57は、深層強化学習と転移学習を組み合わせて使用し、複数のタスクを学習することで汎化性能を向上させます. また、Agent57は、探索と利用のトレードオフを管理するための進化戦略を導入し、高いパフォーマンスを達成しました. 

NAF (Normalized Advantage Functions)は、強化学習において連続的な行動空間での制御を扱うための手法です. NAFは、行動価値関数を近似するためにニューラルネットワークを使用し、状態と行動の特徴を正規化することで収束性と性能を向上させます. NAFは、連続的な制御タスクにおいて効果的な行動選択を実現するための新規性を持っています. 

GPS (Guided Policy Search)は、制約つきの強化学習問題を解決するための手法です. GPSは、トラジェクトリ最適化を用いて方策を学習し、制約条件を満たすための安定かつ効率的な行動選択を実現します. GPSの新規性は、最適化のためのモデルを用いずにデータから直接方策を学習する点にあります. 

それぞれの手法の課題としては、DDPGは収束性の問題や過剰評価の傾向があります. Rainbowは、各手法の組み合わせによる計算コストの増加やパラメータの調整の難しさがあります. Agent57は、多様なタスクの訓練において十分な収束性を達成するための課題があります. NAFは、高次元の行動空間での収束性の向上が課題です. GPSは、制約条件の設計や最適化の効率性の向上が課題となります. 


#### 方策ベース (方策ベース＋価値ベース)
方策勾配法

Actor Critic

soft actor critic

方策勾配法（Policy Gradient）は、強化学習における方策（行動選択の戦略）を直接最適化する手法です. この手法では、エージェントが環境との相互作用を通じて得られる報酬を最大化するために、方策のパラメータを更新します. 

Actor-Criticは、方策勾配法の一種であり、方策（Actor）と価値関数（Critic）の2つのモデルを組み合わせて学習を行います. Actorは方策モデルであり、状態に対して行動を生成します. Criticは状態価値関数や行動価値関数などの価値関数モデルであり、エージェントの行動の価値を評価します. Actorは方策の改善を目指して方策勾配法によって学習し、Criticは方策の評価や学習の補助として使用されます. 

Soft Actor-Critic（SAC）は、Actor-Critic手法の一種であり、特に連続的な行動空間での強化学習において効果的です. SACは、最適化の際にエントロピー項を利用することで、探索と利用のトレードオフを調整します. エントロピー項は方策の確率分布のランダム性を保持し、探索を促進します. SACは、高いパフォーマンスと安定性を実現し、データ効率性を向上させることができます. 

Soft Actor-Criticは、連続的な行動空間における方策最適化において、安定性と探索のバランスを取るための新規性を持っています. エントロピー項によるランダム性の導入により、探索性と収束性の両方を兼ね備えた学習を実現し、高度な制御タスクにおける優れたパフォーマンスを達成します. 


## transformerベース
### CoBERL: Contrastive BERT for Reinforcement Learning 
https://openreview.net/forum?id=sRZ3GhmegS
CoBERLはBertをもとにしたアルゴリズムであり、新しい contrastive loss とLSTMトランスフォーマーを組み合わせた新しいアーキテクチャであり、このモデルを使用することでデータ効率性の課題を解決しています. Atariゲームにおいて57ゲーム中49ゲームで人間のスコアを上回った

### Regularizing Action Policies for Smooth Control with Reinforcement Learning

深層強化学習（RL）で訓練されたコントローラーの実用性における重要な問題は、RLポリシーによって学習されたアクションの滑らかさの欠如です. この傾向は、制御信号の振動という形で現れ、制御の悪化、高い電力消費、そして不必要なシステムの摩耗を引き起こすことがあります. 本研究では、アクションポリシーの滑らかさを改善する効果的かつ直感的な正則化手法であるConditioning for Action Policy Smoothness（CAPS）を提案します. CAPSは、ニューラルネットワークコントローラーの学習された状態からアクションへのマッピングの滑らかさを一貫して向上させる効果があり、制御信号の高周波成分が除去されることで表れます. 実システムでのテストでは、クアッドロータードローンのコントローラーの滑らかさの改善により、ほぼ80％の電力消費削減が実現され、安定したフライト可能なコントローラーの訓練が行われました. プロジェクトのウェブサイトはこちら：[リンク先のURLを記載]


## 今後の課題と発展

マルチタスク

複雑な制御エージェントであるロボットは、しばしば複数の並行タスクに同時に取り組むことがあります. 深層強化学習は、複数のタスクを同時に学習する可能性も提供しています（Mujika、2016）. Mirowskiら（2016）は、目標指向のナビゲーションと深度予測のための深層強化学習モデルを構築しました. このモデルは、LSTM構造を組み合わせることで、ループクロージャーの学習さえも可能です. 深層学習に基づくマルチタスク制御は、探索のために多くの基礎的な実験を行う価値があります. 

深層逆強化学習

逆強化学習（IRL）（Ngら、2000; Abbeel and Ng、2004）は、最適方策に基づく示された行動を通じて報酬関数または制御ポリシーを学習することを目指しています. Wulfmeierら（2015）は、IRLのための最大エントロピー範疇に基づいた報酬関数を訓練するために深層アーキテクチャを使用しました. これは、移動体の経路計画のためのコストマップ学習に成功裏に適用されました（Wulfmeierら、2016）. Finnら（2016）は、高次元ロボットシステムに対して深層逆最適制御を通じて皿の配置などの振る舞いを学習させました. 深層モデルの抽出能力により、深層IRLは生の人間のデモンストレーションから効率的なコストマップを抽出するなど、デモンストレーションからの学習に多くの利点をもたらすことがあります. 

深層学習制御の汎化

深層学習は、訓練時と同様の制約条件にのみ適応できるという一般化の問題についても批判されています. 例えば、先行研究（Tai and Liu、2016b）では、モバイルロボットの視覚ホーミング問題において、深層学習モデルがホーミングベ

クトルの予測に使用されました. ホーミングベクトルは、従来は視覚サーボ制御アルゴリズムに基づいて動機付けられていました（Liuら、2010年、2012年、2013年）. 結果（Tai and Liu、2016b）は、訓練時と同じターゲットを使用すると、予測されるホーミングベクトルが非常に正確であることを示しています. しかし、ランダムに選択された画像ペアでは、それらの相対ベクトルを予測するための入力として使用することはできません. モバイルロボット制御のための深層学習モデルを一般化する方法はまだ課題となっています. 


### interactive imitation learning
- file:///home/owner/Downloads/Interactive_Imitation_Learning_in_Robotics_A_Surve.pdf
この論文では、IIL（インタラクティブイミテーションラーニング）の文献で最も関連性のある研究を調査し、ロボットへの教示やロボットアプリケーションでの潜在的な利点を持つものに焦点を当てます. 近年、多くの研究が、非技術的なバックグラウンドを持つエンドユーザーがロボットシステムのプログラムや操作を可能にするためのこれらの手法の潜在能力を示しています. これら学習戦略の増分的な要素は、方法の使いやすさと得られるポリシーのパフォーマンスレベルにおいて、従来のILに比べて肯定的な影響を与えます. 本論文は、概念や関連する問題の理解を容易にするためのフィールドの構造を提供します. この構成は、新しい研究者の学習曲線を加速するだけでなく、確立されたIIL実践者の理解と展望を向上させるのに役立つでしょう. 

まず、異なる著者が使用する用語の曖昧な定義に関する問題を取り上げ、統一された用語とそれに基づく学習スキームの分類を提案します. これにより、教師から学ぶ方法がIILと見なされるかを具体的に指定することができます. 我々は、教師が相互作用中に提供する情報、学習エージェントが相互作用中に抽出およびモデル化する情報、学習のために得られたデータの取り扱い方、教師と学習者とのコミュニケーションに使用されるインタフェース、IILとRLの関係、獲得した知識を抽象化するために使用されるモデル表現、ループ内に人間がいる場合に考慮すべき事項、その経験を評価する方法、およびその他の考慮事項など、さまざまな重要な側面に応じてアルゴリズムをグループ化します. 

その後、IILメソッドの実験設計に使用できるほとんどのベンチマークと、この種の戦略で取り組まれた最も関連性のあるアプリケーションを紹介します. これらのすべての側面は、方法の選択、設計、実装、またはテストを行う際に考慮する必要があります. 最後に、この研究分野の研究者が直接または間接的にエージェントの学習パフォーマンスと教師の経験を向上させるために取り組む必要があるいくつかの課題について議論します. これらは、研究コミュニティが達成を目指す一般的な目標と見なすことができます. 



## 参考文献
- ![](https://www.researchgate.net/publication/364987581/figure/fig2/AS:11431281094048837@1667358903624/Relationship-between-different-sets-of-learning-paradigms-related-to-the-scope-of-this.png)
  - https://www.researchgate.net/publication/364987581_Interactive_Imitation_Learning_in_Robotics_A_Survey
- A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation
   - https://arxiv.org/pdf/1612.07139.pdf
- https://www.frontiersin.org/articles/10.3389/frobt.2021.777363/full
- Interactive Imitation Learning in Robotics: A Survey
  - https://arxiv.org/abs/2211.00600
- https://www.skillupai.com/blog/tech/dl-papers-2022-4/
- http://karel.tsuda.ac.jp/lec/DeepLearning/dl_book19/colab/dl_book19_ch10.html
- https://qiita.com/aiueola/items/d195532251f482571b1d